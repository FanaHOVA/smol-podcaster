**Alessio**:  Hey everyone, welcome to the Latent Space Podcast.  This is Alessio, partner and CTO of Residence at Decibel Partners,  and I'm joined by my co-host Swiggs, writer and editor of Latent Space. [00:00:12]

**Swyx**:  Hey, and we are here with Tianqi Chen, or TQ as people call him,  who is assistant professor in ML computer science at CMU, Carnegie Mellon University,  also helping to run Catalyst Group, also chief technologist of OctoML.  You wear many hats. Are those your primary identities these days? [00:00:37]

**Tianqi**:  Of course, of course. I'm also very enthusiastic open source,  so I'm also VP and PMC member of the Apache TVM project and so on,  but yeah, these are the things I've been up to so far. [00:00:50]

**Swyx**:  Yeah, so you also created Apache TVM, XGBoost, and MXNet.  And we can cover any of those in any amount of detail.  But maybe what's one thing about you that people might not learn from your official bio or LinkedIn,  like, you know, on a personal side? [00:01:07]

**Tianqi**:  Something on a personal side? Okay.  Let me say, yeah, so normally when I do, I really love coding,  even though I'm trying to run all those things.  So one thing that I keep a habit on is I try to do sketchbooks.  Like, I have a book, like real sketchbooks to draw down the design diagrams,  and the sketchbooks, I keep sketching over the years,  and now I have like three or four of them.  And it's kind of usually a fun experience of thinking the design through  and also seeing how open source project evolves,  and also looking back at the sketches that we had in the past to say, you know,  oh, these ideas really turn into code nowadays. Yeah. [00:01:49]

**Alessio**:  Nice. How many sketchbooks did you get through to build all this stuff?  I mean, if one person alone built one of those projects,  it would be a very accomplished engineer.  Like, you built like three of these?  Yeah.  Yeah. What's that process like for you?  You know, like, is the sketchbook like the start,  and then you think about the code or like, yeah. [00:02:11]

**Tianqi**:  Yeah. So usually I start sketching on high-level architectures,  and also in a project that works for over years,  we also start to think about, you know, new directions,  like of course generative AI language model comes in, how it's going to evolve.  So normally I would say it takes like one book a year, roughly at that rate.  And it's usually fun to – I find it's much easier to kind of sketch things out,  and that gives more like a high-level architectural guide  for some of the future items. Yeah. [00:02:45]

**Swyx**:  Have you ever published these sketchbooks?  Because I think people would be very interested,  at least on a historical basis, like this is the chart where XGBoost was born, you know? [00:02:55]

**Alessio**:  Yeah, not really. [00:02:56]

**Tianqi**:  So actually I haven't sketched since – I started sketching like after XGBoost,  so that's a kind of missing piece.  But a lot of design details in TVM are actually part of the books  that I try to keep a record of, yeah. [00:03:12]

**Swyx**:  We'll try to publish them and publish something in the show notes.  Maybe you can grab a little snapshot for visual aid. [00:03:20]

**Alessio**:  Sounds good, yeah.  And yeah, talking about XGBoost, so a lot of people in the audience  might know it's a gradient boosting library, probably the most popular out there.  And it became super popular because many people started using them  in machine learning competitions.  And I think there's a whole Wikipedia page of all state-of-the-art models  that use XGBoost, and it's a really long list.  When you were working on it – so we just had Tri Dao,  who's the creator of FlashAttention on the podcast,  and I asked him this question.  It's like, when you were building FlashAttention,  did you know that almost any transform race model would use it?  And so I asked the same question to you when you were coming up with XGBoost.  Could you predict it would be so popular?  Or what was the creation process?  And when you published it, what did you expect? [00:04:11]

**Tianqi**:  We have no idea.  Actually, the original reason that we built that library  is that at that time, deep learning just came out.  That was the time where AlexNet just came out.  And one of the ambitious missions that myself and my advisor,  Carl Skelstrom, then is we want to think about,  try to test the hypothesis.  Can we find alternatives to deep learning models?  Because then there are other alternatives,  like support vector machines, linear models,  and, of course, tree-based models.  And our question was, if you build those models  and feed them with big enough data,  because usually one of the key characteristics of deep learning  is taking a lot of data,  will you be able to get the same amount of performance  as those deep learning networks?  And that's a hypothesis we're setting out to test.  Of course, if you look at now, that's a wrong hypothesis.  But as a byproduct, what we find out  is that most of the gradient-boosting library out there  is not efficient enough for us to test that hypothesis.  So I happened to have quite a bit of experience  in the past of building gradient-boosting trees  and their variants.  So effectively, XGBoost was kind of like a byproduct  of that hypothesis testing.  And then, you know, we find maybe, you know,  people will find it useful.  I also...  So at that time, I'm also competing,  building data science challenges.  Like, I worked on KDDCup,  and then Kaggle come become big, right?  So I kind of think, you know,  maybe it's becoming useful to others.  One of my friends convinced me  to try to do a Python binding of it.  That tends to be, like, a very good decision, right?  To effectively...  Usually, when I build it, we feel like, you know,  maybe a command-line interface is OK.  And then we had a Python binding.  We have R bindings.  And then I realized, you know,  it start to getting interesting,  and people start contributing different perspectives,  like visualization and so on.  So we start to, you know, push a bit more  on to, you know, building distributed support  to make sure it works on animal platform and so on.  And even at that time point,  when I talked to Carlos, my advisor, later,  he said he never anticipated  that we'll get to that level of success.  And actually, why I push for gradient-boosting,  it's interesting.  At that time, he also disagreed.  He thinks that maybe we should go for kernel machines then.  And it turns out, you know,  actually, we are both wrong in some sense,  and deep neural network was the king in the hill.  But at least the gradient-boosting direction  got into something fruitful. [00:06:53]

**Alessio**:  Yeah.  Interesting.  What was the...  I'm always curious when it comes to these improvements.  Like, what's the design process  in terms of, like, coming up with it,  and how much of it is collaborative  with, like, other people that you're working with  versus, like, trying to be, you know,  obviously, in academia, it's, like, very paper-driven,  kind of research-driven. [00:07:15]

**Tianqi**:  So I would say  the extra-boost improvement at that time point  was more on, like, you know,  I'm trying to figure out, right?  But it's combining lessons.  Like, before that,  I did work on some of the other libraries  on, like, matrix factorization.  That was, like, my first open-source experience.  Nobody knew about it.  Because you will find...  Likely, if you go and try to search  for the package of SVD feature,  you will find some, like, SVN repo somewhere.  It's actually being used  for some of the recommender system packages.  But then...  So I'm trying to apply some of the previous lessons there  and trying to combine them.  The later projects, like MXNet and then TVM,  is much, much more collaborative,  in a sense that...  But, of course, extra-boost has become bigger, right?  So when we started that project myself,  and now we have...  It's really amazing to see people coming in.  Like, we have Michael, who was a lawyer,  and now he works on the AI space as well,  on contributing visualizations.  Then we have people from our community  contributing different things.  So extra-boost, even today, right?  There is a community of committers  driving the project.  So it's definitely something collaborative  and moving forward on getting some of the things  continuously improved for our community. [00:08:43]

**Alessio**:  Yeah. [00:08:44]

**Alessio**:  Let's talk a bit about TVM too,  because we got a lot of things to run through  in this episode.  So extra-boost...  Go ahead, Sean. [00:08:58]

**Swyx**:  I'll flag that at some point.  I'll flag that at some point.  I'd love to talk about this comparison  between extra-boost or tree-based type AI  or machine learning compared to deep learning.  Because I think there is a lot of interest  around, I guess, merging the two disciplines, right?  Of course.  We can talk more about that.  I don't know where to insert that, by the way.  So we can come back to it later. [00:09:25]

**Alessio**:  Yeah, no, I'm ready now. [00:09:28]

**Tianqi**:  Actually, when I said when we test hypothesis,  the hypothesis is kind of...  I would say it's partially wrong,  because the hypothesis we want to test now is,  can you run tree-based models on image classification tasks,  where deep learning is certainly the no-brainer [00:09:42]

**Alessio**:  right now, today, right? [00:09:43]

**Tianqi**:  But if you try to run it on tabular data,  still you'll find that most people opt for tree-based models.  And there's a reason for that, in a sense,  that when you are looking at tree-based models,  the decision boundaries are naturally rules  that you are looking at, right?  And they also have nice properties,  like being able to be agnostic to scale of input  and be able to automatically compose feature together.  So I know there are attempts.  There are attempts on building neural network models  that works for tabular data,  and I also sometimes follow them.  I do feel like it's good to have a bit of diversity  in the modeling space.  And I feel like...  Actually, when we're building TVM,  we build cost models for the programs.  And actually, we are using XGBoost for that as well.  So I still think tree-based model is going to be quite relevant,  because first of all, it's really to get it work out of box.  And also, you will be able to get a bit of interoperability  and control monotonicity and so on.  So yes, it's still going to be relevant.  And also, I also sometimes keep coming back to think about  are there possible improvements that we can build  on top of these models?  And definitely, I feel like it's a space  that can have some of the potential in the future. [00:11:09]

**Swyx**:  Are there any current projects that you would call out  as promising in terms of merging the two, I guess, directions? [00:11:18]

**Tianqi**:  I think there are projects that tries to bring  a transformer-type model for tabular data.  I don't remember specifics of them,  but I think even nowadays, if you look at what people are using,  like tree-based models still one of their toolkit.  So I think maybe eventually, it's not even a replacement.  It will be just an ensemble of models that you can call. [00:11:45]

**Alessio**:  Perfect. [00:11:48]

**Swyx**:  Over to you, Les. [00:11:52]

**Alessio**:  So next step, about three years after XGBoost,  you built this thing called TVM,  which is now a very popular compiler framework for models.  Let's talk about...  So this came out about at the same time as ONIX.  So I think it would be great if you could maybe give  a little bit of an overview of how the two things work together  because it's kind of like the model, then goes to ONIX,  then goes to the TVM.  But I think a lot of people don't understand that nuances. [00:12:21]

**Tianqi**:  Yeah. So I can get a bit of backstory on that.  So actually, that's kind of an ancient history.  Before XGBoost, I worked on deep learning  for like two years or three years.  So I get a master before I start my PhD.  And during my master, my thesis focused on applying  what I call the convolutional restricted Boltzmann machine  for ImageNet classification.  That is the thing I'm working on.  And that was before AlexNet moment.  So effectively, I had to handcraft NVIDIA CUDA kernels  on, I think, a GTX 2070 card.  I have a 22070 card.  And it took me about, I think, six months  to get one model working.  And eventually, that model is not so good.  And we should have picked a better model.  But that was like an ancient history  that really got me into this deep learning field.  And of course, eventually, we find it didn't work out.  So in my master, in that, I ended up  working on recommender system, which got me a paper.  And I applied and got a PhD.  But I always want to come back to work  on the deep learning field.  So after XGBoost, I think I started  to work with some of the folks on this particular MXNet.  At that time, it was like the frameworks  are CAFE, Ciano, Torch.  PyTorch haven't yet come out.  And we're really working hard to optimize  for performance on GPUs.  And we find that at that time, I find  it's really hard, even for NVIDIA GPU.  It took me six months.  And then it's amazing to say on different hardwares  how hard it is to go and optimize  code for the platforms that are interesting.  So that gets me thinking.  Can we build something more generic and automatic  so that I don't need an entire team of so many people  to go and build those frameworks?  So that's the motivation of starting working on TVM.  So the goal there is really to lower  the bar of machine learning engineering needed  to support deep learning models on the platforms  that we are interested in.  Yeah, so I think it started a bit earlier than ONNX.  But when it got announced, I think  it's in a similar time period at that time.  So yeah, so overall, how it works  is that TVM, you will be able to take  a subset of machine learning program  that are represented in what we call computational graph.  Nowadays, we can also represent it as a loop level program.  We ingest from our machine learning models.  Usually, you have model formats, ONNX.  Or in PyTorch, they have FX tracer  that allows you to trace the FX graph.  And then it goes through TVM.  We also realized that, well, yes, it  needs to be more customizable.  So we'll be able to perform some of the compilation  optimizations, like fusion operator together,  doing smart memory plannings, and more importantly,  generate low-level code.  So that works for NVIDIA and also  is portable to other GPU backends,  and even non-GPU backends out there.  So that's a project actually has been my primary focus  over the past few years.  And it's great to see how it started from where,  I think, we are the very early initiator  of machine learning compilation.  I remember there was a visiting day.  One of the students asked me, are you still  working on different frameworks?  I tell them that I'm working on ML compilation.  And they said, OK, compilation.  That sounds very ancient.  It sounds like a very old field.  And why are we working on this?  And now it's starting to get more attractions,  like if you say Torch Compile and other things.  I'm really glad to see this field starting picking up.  And also, we have to continue innovating here. [00:16:23]

**Alessio**:  I think the other thing that I noticed  is it's kind of like a big jump in terms of area of focus  to go from XGBoost to TVM.  It's kind of like a different part of the stack.  Why did you decide to do that?  And I think the other thing about compiling  to different GPUs and eventually CPUs too,  did you already see some of the strain that models could have,  just being focused on one runtime,  only being on CUDA and that?  How much of that went into it? [00:17:02]

**Tianqi**:  I think it's less about trying to get impact,  more about wanting to have fun.  I like to hack code.  And I had great fun hacking CUDA code.  And of course, being able to generate CUDA code is cool.  But now, after being able to generate CUDA code, OK,  by the way, you can do it on other platforms.  Isn't that amazing?  So it's more of that attitude to get me started on this.  And also, when I look at different researchers,  myself is more like, I think, a problem solver type.  So I like to look at a problem and say, OK,  what kind of tools we need to solve that problem?  So regardless, it could be building better models.  For example, when we build extra boots,  we build certain regularizations into it so that it's more robust.  It also means building system optimizations,  writing low-level code, maybe trying to write assembly  and build compilers and so on.  So as long as they solve the problem,  definitely go and try to do them together.  And I also say it's a common trend right now.  If you want to be able to solve machine learning problems,  it's no longer an algorithm layer.  You need to solve it from both algorithm, data,  and systems angle.  And this entire field of machine learning system, I think,  is emerging.  And there's now a conference around it.  And it's really good to see a lot more people are  starting to look into this. [00:18:30]

**Swyx**:  Are you talking about ICML or something else? [00:18:32]

**Tianqi**:  So machine learning and systems.  So not only machine learning, but machine learning and systems.  So there's a conference called MLsys.  It's definitely a smaller community than ICML,  but I think it's also an emerging and growing community  where people are talking about what are the implications  of building systems for machine learning,  and how do you go and optimize things around that  and co-design models and systems together. [00:18:57]

**Swyx**:  Yeah.  Are you also heavily involved in MLsys? [00:19:02]

**Tianqi**:  Yeah.  So I was the PC chair last year. [00:19:05]

**Alessio**:  Right.  OK. [00:19:07]

**Swyx**:  Sorry.  I just saw it on your CV.  Yeah.  And you were area chair for ICML and NeurIPS as well.  So you've just had a lot of conference and community  organization experience.  Is that also an important part of your work?  Yeah. [00:19:19]

**Tianqi**:  Well, it's kind of expected for academic.  If I hold an academic job, I need to do services  for the community. [00:19:26]

**Alessio**:  Yeah.  Yeah. [00:19:28]

**Swyx**:  OK.  Great.  Your most recent venture in MLsys is going to the phone  with MLC LLM.  You announced this in April, and I have it on my phone.  It's great.  I'm running Lama 2, Vicuña.  I don't know what other models you offer.  But maybe just kind of describe your journey into MLC.  And I don't know how this coincides with your work at CMU.  Is that some kind of outgrowth?  Yeah. [00:20:02]

**Tianqi**:  MLC is kind of like the effort.  I think it's more like a focused effort that we want  in the area of machine learning compilation.  So it's kind of related to what we built in TVM.  So when we built TVM, it was five years ago.  And a lot of things happened.  We built the end-to-end machine learning compiler that works,  the first one that works.  But then we captured a lot of lessons there.  So then we are building a second iteration called TVM Unity  that allows us to be able to allow ML engineers  to be able to quickly capture the new model  and how we demand building optimizations for them.  And MLC LLM is kind of like an MLC.  It's more like a vertical-driven organization  that we go and build tutorials and go and build projects  like LLM to solutions to really show,  okay, you can take machine learning compilation technology  and apply it and bring something fun forward.  So yes, it runs on phones, which is really cool.  But the goal here is not only making it run on phones.  The goal is making it deploy universally.  So we do run on Apple M2 Max, the 17-billion models.  Actually, on a single batch inference, more recently on CUDA,  we get, I think, the most best performance  we can get out there already on the 4-bit inference.  And actually, as I alluded earlier before the podcast,  we just had a result on AMD.  And on a single batch, actually, we can get the latest AMD GPU.  It is the consumer card.  It can get to about 80% of the 4019s,  so NVIDIA's best consumer card out there.  So it's not yet on par.  But thinking about how our diversity  and what you can enable and the previous things  you can get on that card, it's really amazing  what you can do with this kind of technology. [00:22:04]

**Alessio**:  Yeah. [00:22:06]

**Swyx**:  So one thing I'm a little bit confused by  is that most of these models are in PyTorch,  but you're running this inside a TVM.  I don't know.  Was there any fundamental change that you needed to do?  Or was this basically the fundamental design of TVM? [00:22:23]

**Tianqi**:  So the idea is that, of course, it comes back  to program representation.  So effectively, TVM have this program representation  called TVM script that contains more computational graph  and operational representation.  So yes, initially, we do need to take a bit of effort  of bringing those models onto the program representation  that TVM support.  Usually, there are a mix of ways,  depending on kind of model you're looking at.  For example, for vision models and stable diffusion models,  usually we can just do tracing.  That takes PyTorch model on the TVM.  And that part is still being robustified so that we  can bring more model in.  On language model tasks, actually what we do  is we directly build some of the model constructors  and try to directly map from Hugging Face models.  The goal is if you have a Hugging Face configuration,  we will be able to bring that in and apply optimization  on them.  So one fun thing about model compilation  is that your optimization don't happen only  at a source language.  For example, if you're writing PyTorch code,  you just go and try to use better fused operator  at a source code level.  Torch compile might help you do a bit of things in there.  In most of model compilations, it not only  happens at beginning stage, but we also  apply generic transformations in between.  Also through a Python API, so you can tweak some of that.  So that part of optimization helps a lot of uplifting  in getting both performance and also portability  on the environment.  And another thing that we do have  is what we call universal deployment.  So if you get the ML program into this TVM script format,  where there are functions that takes in tensor and output  tensor, we will be able to have a way to compile it.  So they will be able to load the function  in any of the language runtime that TVM supports.  So for example, you could load it in JavaScript.  And that's a JavaScript function that you  can take in tensors and output tensors  if you're loading Python, of course, and C++ and Java.  So the goal there is really bring the ML model  to the language that people care about  and be able to run it on a platform they like. [00:24:43]

**Swyx**:  It strikes me that I talk to a lot of compiler people,  but you don't have a traditional compiler background.  I don't.  You're kind of inventing your own discipline  called machine learning compilation, or MLC.  Do you think that this will be a bigger field going forward? [00:25:02]

**Tianqi**:  Well, first of all, I do work with people  working on compilation as well.  So we're also taking inspirations  from a lot of early innovations in the field, like for example,  TVM.  Initially, we take a lot of inspirations from Halide,  which is this image processing compiler.  And of course, since then, we have  evolved quite a bit to focus on the machine learning-related  compilations.  If you look at some of our conference publications,  you will find the machine learning compilation  is already kind of a subfield.  So if you look at papers in both machine learning venues,  the MLCs conferences, of course, and also system venues,  every year there will be papers around machine learning  compilation.  And in the compiler conference called CGO,  there's a C4ML workshop that also kind of trying  to focus on this area.  So definitely, it's already starting to gain traction  and becoming a field.  I wouldn't claim that I invented this field,  but definitely, I help to work with a lot of folks there  and try to bring a perspective.  Of course, trying to learn a lot from the compiler  optimizations, as well as trying to bring in knowledges  in machine learning and assistance together in here. [00:26:23]

**Alessio**:  Very cool.  So we had George Hotz on the podcast a few episodes ago,  and he had a lot to say about AMD and their software.  That's right.  So when you think about TVM, are you still  restricted in a way by the performance  of the underlying kernel, so to speak?  So if your target is like a CUDA runtime,  you still get better performance no matter?  Like TVM kind of helps you get there,  but then that level you don't take care of, right? [00:26:55]

**Tianqi**:  There are two parts in here.  So first of all, there is the lower level runtime,  like CUDA runtime.  And then, actually, for NVIDIA, a lot of the mood  came from their libraries, like Cutlass, UDN,  those library optimizations.  And also, for specialized workloads,  actually, you can specialize them.  Because a lot of cases, you will find that if you go  and do benchmarks, it's very interesting.  Like two years ago, if you try to benchmark ResNet,  for example, usually the NVIDIA library  gives you the best performance.  It's really hard to beat them.  But as soon as you start to change the model to something,  maybe a bit of a variation of ResNet,  not for the traditional ImageNet detections,  but for latent detection and so on,  there will be some room for optimization.  Because people sometimes overfit to benchmarks.  These are people who go and optimize things, right?  So people overfit to benchmarks.  So that's the largest barrier, like being  able to get to low-level kernel libraries, right?  And in that sense, the goal of TVM  is actually we try to have a generic layer to both,  of course, leverage libraries when available,  but also be able to automatically generate  libraries when possible.  So in that sense, we are not restricted by the libraries  that they have to offer.  That's why, for example, we will be able to run Apple M2,  a WebGPU where there's no library available,  because we are kind of like automatic generating libraries.  And it does make it easier to support  like a less well-supported hardware, right?  Like, for example, WebGPU is one example  from a runtime perspective.  AMD, I think, before their locom driver  was not very well supported.  And recently, they are getting good.  But even before that, we'll be able to support AMD  through this GPU graphics backend called Vulkan, which  is not as performant, but gives you  a decent portability across those hardware. [00:29:05]

**Alessio**:  Great.  And I know we got other MLC stuff to talk about,  like WebLLM.  But I want to wrap up on the optimization  work that you're doing.  So there's kind of four core things, right?  Kernel fusion, which we talked a bit about  in the flash attention episode and the tiny graph one.  Memory planning and loop optimization,  I think those are pretty self-explanatory.  I think the one that people have the most questions about [00:29:30]

**Swyx**:  is like-  Can you quickly explain those? [00:29:33]

**Alessio**:  Yeah, go for it. [00:29:34]

**Tianqi**:  Yeah, so there are kind of different things, right?  Kernel fusion means that if you have an operator  like convolutions, or in the case of transformer,  like MOP, you have other operators follow that.  You don't want to launch two GPU kernels.  You want to be able to put them together in a smart way.  And as a memory planning, it's more about, hey,  if you run like Python code, every time when  you generate a new array, you are effectively  allocating a new piece of memory.  Of course, PyTorch and other framework  try to optimize for you.  So there is a smart memory allocator behind the scene.  But actually, in a lot of cases, it's  much better to statically allocate and plan  everything ahead of time.  And that's where a compiler can come in.  We need to, first of all, actually,  for language model, it's much harder because dynamic shape.  So you need to be able to, what we call,  symbolic shape tracing.  So we have a symbolic variable that  tells you the shape of the first tensor is n by 12.  And the shape of the third tensor is also n by 12.  Or maybe it's n times 2 by 12.  Although you don't know what n is,  but you will be able to know that relation  and be able to use that to reason about fusion  and other decisions.  So besides this, I think loop transformation  is quite important.  And it's actually non-traditional.  Originally, if you simply write a code  and you want to get a performance, it's very hard.  For example, if you write matrix multiplier,  the simplest thing you can do is you do for i, j, k, c, i, j,  plus, equal, a, i, k, times c, i, k.  But that code is 100 times slower than the best available  code that you can get.  So we do a lot of transformation,  like being able to take the original code,  trying to put things into shared memory,  and making use of tensor calls, making use of memory copies.  And actually, all these things, we also  realize that we cannot do all of them.  So we also make the ML compilation framework  as a Python package, so that people  will be able to continuously improve  that part of engineering in a more transparent way.  So we find that's very useful, actually,  for us to be able to get good performance very quickly  on some of the new models.  Like when Lamato came out, we'll be  able to go and look at the whole, here's the bottleneck,  and we can go and optimize those. [00:31:57]

**Alessio**:  Yeah.  Yeah.  And then the fourth one being weight quantization.  So I think that's one of the most,  everybody wants to know about that.  And just to give people an idea of the memory saving,  if you're doing, like, FB32 is like four bytes per parameter,  like int8 is like one byte per parameter.  So you can really shrink down the memory footprint.  What are some of the trade-offs there?  How do you figure out what the right target is?  And what are the precision trade-offs, too?  Yeah. [00:32:29]

**Tianqi**:  I think right now, a lot of people,  we also mostly use int4 now for our language models.  So that really shrinks things down a lot.  And more recently, actually, we started  to think that, at least in MOC, we  don't want to have a strong opinion on what kind  of quantization we want to bring,  because there are so many researchers in the field.  So what we can do is we can allow developers to customize  the quantization they want, but we still  bring the optimum code for them.  So we are working on this item called  bring your own quantization, effectively,  to be able to hopefully, you know,  MOC will be able to support more quantization format.  And definitely, I think there's an open field that's  being explored.  Like, can you bring more sparsities?  Can you quantize activations as much as possible, and so on?  And it's going to be something that's going  to be relevant for quite a while. [00:33:19]

**Alessio**:  Yeah. [00:33:19]

**Swyx**:  You mentioned something I wanted to double back on,  which is most people use int4 for language models.  This is actually not obvious to me.  Are you talking about the GGML type people,  or even the researchers who are training  the models also using int4? [00:33:36]

**Tianqi**:  Oh, sorry.  So I'm mainly talking about inference, not training,  right?  So when I'm doing training, of course, int4 is harder, right?  Maybe you could do some form of mixed type positions  for inference.  I think int4 is kind of like a lot of cases  that you will be able to get away with int4 in a lot of cases.  And actually, that does bring a lot of savings  in terms of the memory overhead, and so on. [00:34:06]

**Alessio**:  Good.  Yeah, that's great.  And so, yeah, let's talk a bit about maybe the GGML.  Then there's Mojo.  How should people think about MLC?  How do all these things play together?  I think GGML is focused on model level re-implementation  and improvements.  Mojo is a language, super sad.  You're more at the compiler level.  Do you all work together?  Do people choose between them? [00:34:36]

**Tianqi**:  So I think in this case, I think it's great to say  the ecosystem becomes so rich with so many different ways.  So in our case, I would say GGML is more  like you are implementing something from scratch in C.  So that gives you ability to go and customize  each of a particular hardware backend.  But then you will need to write from CUDA kernels,  and you write optimally for AMD, and so on.  So the kind of engineering effort  is a bit more broadened, in that sense.  Mojo, I have not looked at specific details yet.  But I think it's good to start to say it's a language.  I believe there will also be machine and compilation  technologies behind it.  So it's good to say interesting place in there.  In the case of MLC, our case is that we do not  want to have an opinion on how, where, which language  people want to develop, deploy, and so on.  And we also realize that actually,  if people want to, so there are two phases.  You want to be able to develop and optimize your model.  By optimization, I mean really bring best CUDA kernels  and do some of the machine learning engineering in there.  And then there's a phase where you want to deploy  as a part of app.  So if you look at the space, you'll  find that GGML is more like, I'm going  to develop and optimize in the C language,  and then most of the low-level languages they have.  And Mojo is that you want to develop and optimize  in Mojo, and you deploy in Mojo.  You deploy in Mojo.  In fact, that's the philosophy they want to push for.  In the ML case, we find that actually,  if you want to develop models, machine learning community  lacks Python.  Python is a language that you should focus on.  So in the case of MLC, we really want  to be able to enable not only be able to just define  your model in Python.  That's very common.  But also do ML optimization, like engineering optimization,  CUDA kernel optimization, memory planning,  all those things in Python that makes you customizable, [00:36:45]

**Alessio**:  and so on. [00:36:46]

**Tianqi**:  But when you do deployment, we realize  that people want a bit of universal flavor.  If you are a web developer, you want JavaScript.  If you are maybe an embedded system person,  maybe you would prefer C++ or C or Rust.  And people sometimes do like Python in a lot of cases.  So in the case of MLC, we really want  to have this vision of you optimize,  build a generic optimization in Python,  and then you deploy that universally  onto the environments that people like. [00:37:20]

**Swyx**:  Very cool.  That's a great perspective and comparison, I guess.  One thing I wanted to make sure that we cover  is that I think you are one of these emerging  set of academics that also very much focus  on your artifacts of delivery.  Something we talked about for three years,  that he was very focused on his GitHub.  And obviously, you treated XGBoost like a product.  That's right, yeah.  And then now you're publishing an iPhone app.  OK, yeah, yeah.  Is this a broader trend of what is this thinking  about academics getting involved in shipping product? [00:38:05]

**Tianqi**:  I think there are different ways of making impact.  Definitely, there are academics that  are writing papers and building insights for people  so that people can build product on top of them.  In my case, I think the particular field  I'm working on, machine learning systems,  I feel like really we need to be able to get it  to the hand of people so that really we see the problem  and we show that we can solve the problem.  And it's a different way of making impact.  And there are academics that are doing similar things  like if you look at some of the people from Berkeley,  a few years they will come up with big open source projects.  Certainly, I think it's just a healthy ecosystem  to have different ways of making impacts.  And I feel like really be able to do open source  and work with open source community is really rewarding  because we have real problem to work on.  When we build our research, actually those research  bring together and people will be able to make use of them.  And we also start to see interesting research challenges  that we wouldn't otherwise see if we're just trying  to do a prototype and so on.  So I'm really into this.  And I feel like it's something that is one interesting way  of making impact, making contributions. [00:39:28]

**Swyx**:  You definitely have a lot of impact there.  And having experience publishing Mac stuff before,  the Apple App Store is no joke.  It is the hardest compilation, human compilation effort.  So one thing that we definitely wanted to cover  is running in the browser.  You have a 70 billion parameter model running in the browser.  That's right.  Can you just talk about how? [00:39:54]

**Tianqi**:  Yeah, of course.  I think there are a few elements that need to come in.  First of all, we do need a MacBook, the latest one, M2 Max,  because you need the memory to be big enough to cover that.  So for a 70 billion model, it takes you about, I think,  50 gigas of RAM.  So the M2 Max, the upper version, will be able to run it.  And it also leverages machine learning compilation.  Again, what we are doing is the same.  Whether it's running on iPhone, on server class GPUs, on AMDs,  or on MacBook, we all go through that same MOC pipeline.  Of course, in certain cases, maybe we'll do a bit of customization  iteration for either ones.  And then it runs on the browser runtime.  There's this package of WebLM, so that will effectively...  So what we do is we will take that original model  and compile it to what we call WebGPU.  And then the WebLM will be able to pick it up.  And the WebGPU is this latest GPU technology  that major browsers are shipping right now.  So you can get it in Chrome for them already.  It allows you to be able to access your native GPUs from a browser.  And then, effectively, that language model  is just invoking the WebGPU kernels through there.  So we...  Actually, when the LATMAR2 came out,  initially we asked the question about,  can you run a 70 billion on a MacBook?  That was the question we were asking.  So first, we actually...  Jin Lu, who is the engineer pushing this,  he got 70 billion on a MacBook.  We had a CLI version.  So in the MOC, you will be able to...  That runs through a metal accelerator.  So, effectively, you use the metal programming language  to get the GPU acceleration.  So we found, OK, it works for the MacBook.  Now we ask, we had a WebGPU backend.  Why not try it there?  So we just tried it out.  And it's really amazing to see everything up and running,  and actually it runs smoothly in that case.  So I do think there are some interesting use cases already in this,  because everybody has a browser.  You don't need to install anything.  Maybe it doesn't make sense to...  I think it doesn't make sense yet  to really run a 70 billion model on a browser,  because you need to be able to download the weight and so on.  But I think we are getting there.  Like, effectively, the most powerful models  you will be able to run on a consumer device.  It's kind of really amazing.  And also, in a lot of cases,  there might be use cases, for example,  if I'm going to build a chatbot that I talk to it  and answers questions,  maybe some of the components, like the voice to text,  could run on the client side.  So there are a lot of possibilities  of being able to have something hybrid  that contains the edge component  or something that runs on our server. [00:42:49]

**Alessio**:  Do these browser models have, then,  a way for applications to hook into them?  So if I'm using...  You can use OpenAI, or you can use the local model.  Of course. [00:42:59]

**Tianqi**:  So right now, actually, we are building...  So there's an NPM package called WebILM, right?  So that you will be able to...  If you want to embed it onto your web app,  you will be able to directly depend on WebILM,  and you will be able to use it.  We are also having a REST API that's OpenAI-compatible.  So that REST API, I think, right now,  it's actually running on native backend,  so that if a CUDA server is faster  to run on native backend.  But also, we have a WebGPU version of it  that you can go and run.  So yeah, we do want to be able to have easier integrations  with existing applications,  and OpenAI API is certainly one way to do that. [00:43:40]

**Alessio**:  Yeah. [00:43:41]

**Swyx**:  Yeah, this is great.  I actually did not know there's an NPM package  that makes it very, very easy to try out and use.  I want to actually...  One thing I'm unclear about is the chronology.  Because as far as I know,  Chrome shipped WebGPU the same time that you shipped WebILM.  Okay, yeah.  So did you have some kind of secret chat with Chrome? [00:44:04]

**Tianqi**:  So the good news is that Chrome is doing a very good job  of trying to have early release.  So although the official shipment of the Chrome WebGPU  is the same time of WebILM,  actually, you will be able to try out WebGPU technology  in Chrome.  There are unstable version called Canary.  I think as early as two years ago,  there was a WebGPU version.  Of course, it's getting better.  So we had TVM-based WebGPU backend two years ago.  Of course, at that time, there's no language models.  It's running on less interesting,  well, still quite interesting models.  And then this year, we really started to see  it's getting mature and performance keeping up.  So we have a more serious push  of bringing the language model compatible runtime  onto the WebGPU. [00:45:00]

**Alessio**:  Yeah. [00:45:02]

**Swyx**:  Would there...  So I think you agree that the hardest part  is the model download.  Has there been conversations about sharing,  like a one-time model download  and sharing between all the apps that might use this API? [00:45:19]

**Tianqi**:  That is a great point.  I think it's already supported kind of already.  In some sense, when we download the model,  WebILM will cache it onto a special Chrome cache.  So if a different web app uses the same WebILM  JavaScript package,  you don't need to re-download the model again.  So there is already something there.  But of course, you have to download the model once  at least to be able to use it. [00:45:43]

**Swyx**:  And that's a pretty long download.  That's a pretty big bite.  Okay.  Yeah.  And then one more thing just in general before...  We're about to zoom out to the OctoAI.  But just the last question is,  you're not the only project working on,  I guess, local models, alternative models.  There's GPC4ALL.  There's OLAMA that just recently came out.  And there's a bunch of these.  What would be your advice to them  on what's a valuable problem to work on  and what is just thin wrappers around GGML?  What are the interesting problems in this space, basically? [00:46:19]

**Tianqi**:  I think making API better is certainly something useful.  In general, one thing that we do try to push very hard on  is this idea of easier universal deployment.  And we are also looking forward to actually  have more integration with MOC.  That's why we're trying to build API like WebRM and other things.  So we're also looking forward to collaborate  with all those ecosystem working support  to bring in models more universally  and be able to also keep up the best performance when possible  in a more push-button way. [00:46:50]

**Alessio**:  Yeah.  Awesome.  So as we mentioned in the beginning,  you're also the co-founder of OctoML.  Recently, OctoML released OctoAI,  which is a compute service.  Basically, it focuses on optimizing model runtimes  and acceleration and compilation.  What has been the evolution there?  So Octo started as kind of like a traditional MLOps tool  where people were building their own models  and you helped them on that side.  And then it seems like now most of the market is shifting  to starting from pre-trained generative models.  Yeah.  What has been that experience for you  and what you've seen the market evolve  and how did you decide to release OctoAI? [00:47:35]

**Tianqi**:  So one thing that we find out is that, you know,  on one hand, it's really easy to go  and get something up and running, right?  So if you start to consider  there's so many possible availabilities  and scalability issues and even integration issues  since becoming kind of interesting and complicated.  So we really want to make sure  to help people to get that part easy, right?  And now a lot of things, if we look at the customers  we talk to and the market,  certainly generative AI is something  that is very interesting.  So that is something that we really hope to help elevate.  And also building on top of technology we build  to enable things, you know, like portability across hours  and you will be able to not worry  about the specific details, right?  So just on getting the model out  and we'll try to work on infrastructure  and other things that helps on the other end, yeah. [00:48:33]

**Alessio**:  And when it comes to getting optimization on the runtime,  I see when I, we run like an early adopters community  and like most enterprises issue is like  how to actually run these models, you know?  Do you see that as like one of the big bottlenecks now?  I think like a few years ago it was like,  well, we don't have a lot of like machine learning talent.  We cannot develop our own models, you know?  Versus now it's like there's these great models  you can use but like I don't know how to run them efficiently. [00:49:03]

**Tianqi**:  That depends on how you define by run then, right?  So on one hand it's easy to download,  even MLC like you download it, you run on a laptop  but then there's also different decisions, right?  What if you are trying to serve a larger user request?  What if that request changes, right?  What if availability of hardware changes?  Like right now it's really hard to get  the latest hardware on media, unfortunately,  because of this like everybody's trying to working on  the things and using the hardware that's out there, right?  So I think they are kind of like when the definition  of run changes, they're kind of like  there are a lot more questions around things.  And also in a lot of cases it's not only about running models,  it's also about being able to leverage,  being able to solve problems around them,  like how do you manage your model locations, right?  And how do you make sure that you get your model  close to your execution environment more efficiently?  So definitely a lot of engineering challenges out there  that we hope to elevate, yeah. [00:50:10]

**Alessio**:  Yeah, go ahead. [00:50:13]

**Tianqi**:  And also if you think about our future,  definitely I feel like right now the technology,  given the technology and kind of hardware availability  we have today, we will need to make use  of all the possible hardware available out there.  And that will include mechanisms for cutting down costs,  bringing something to edge and cloud in a more natural way.  So I feel like it's still, this is a very early stage  of where we are, but it's already good to say  like a lot of interesting progress, yeah. [00:50:43]

**Alessio**:  Yeah, that's awesome.  And I would love, I don't know how much  you can go in depth into it, but what does it take  to actually abstract all of this from the end user?  They don't need to know what GPUs you run,  what cloud you're running them on.  You take all of that away.  What was that like as an engineering challenge? [00:51:02]

**Tianqi**:  So I think there are engineering challenges.  In fact, first of all, you will need to be able to support  all the kind of hardware back end you have, right?  On one hand, if you look at the media library,  you'll find very surprisingly, not too surprisingly,  most of the latest libraries works well on the latest GPU.  But there are other GPUs out there in the cloud as well.  So certainly being able to have know-hows  and being able to do model optimization is one thing, right?  Also infrastructures on being able to scale things up,  locate models, and in a lot of cases we do find  that on typical models, it also requires  kind of vertical iterations.  So it's not about build a silver bullet,  and that silver bullet is going to solve all the problems.  It's more about we're building a product,  we work with the users, and we find out  there are interesting opportunities in a certain point,  and our engineer will go and solve that.  And it will automatically reflect it in the service. [00:51:57]

**Alessio**:  Awesome.  Yeah, we can jump into the landing ground until,  I don't know, Sean, if you have more questions,  or TQ, if you have more stuff you wanted to talk about  that we didn't get a chance to touch on. [00:52:10]

**Tianqi**:  Yeah, we have talked a lot, so yeah. [00:52:13]

**Swyx**:  Yeah, I always would like to ask,  and we can edit this out if there's nothing,  but do you have a commentary on other parts of AI and ML  that is interesting to you? [00:52:24]

**Tianqi**:  Yeah, other parts.  So right now I think one thing  that we are really pushing hard for  is this question about how far can we bring in open source.  I'm kind of like a hacker,  and I really like to put things together.  So I think it is unclear in the future  of what the future of AI looks like.  On one hand, it could be possible  that you just have a few big players,  you just try to talk to those bigger language models,  and that can do everything, right?  On the other hand, one of the things  that I'm kind of willing, academic,  I'm really excited pushing for,  that's one reason why I'm pushing for MLC,  is that can we build something  where you have different models,  you have personal models  that knows the best movie you like,  but you also have bigger models  that maybe know more,  and you get those models interact with each other,  and be able to have a wide ecosystem  of AI agents that helps each person  while still be able to do things like personalization.  Some of them can run locally,  some of them, of course, running on a cloud,  and how do they interact with each other?  So I think that is,  we are in a very exciting time  where the future is yet undecided,  but I feel like there are some things  we can do to shape that future as well, yeah. [00:53:49]

**Alessio**:  Awesome.  Do you have an idea, [00:53:53]

**Swyx**:  sorry, one more thing,  which is something I'm also pursuing,  which is, this kind of goes back into predictions,  but also back in your history,  do you have any idea,  or are you looking out for anything post-transformers  as far as architecture is concerned? [00:54:10]

**Tianqi**:  I think in a lot of these cases,  you can find there are already promising models  for long context,  where there are space-based models  where a lot of some of my colleagues,  for example, Albert,  who he worked on this HIPPO models,  and there is an open source version called RWKB,  these recurrent models  that allows you to summarize things.  Actually, we are bringing RWKB to MOC as well,  so maybe you will be able to see one of the models soon. [00:54:39]

**Swyx**:  We actually recorded an episode  with one of the RWKB core members.  Okay.  It's unclear,  because there's no academic backing,  it's just open source people. [00:54:48]

**Tianqi**:  Oh, I see. [00:54:50]

**Swyx**:  So you like the merging of recurrent networks  and transformers. [00:54:55]

**Tianqi**:  I think there are,  I do love to see this model space continue growing,  and I feel like in a lot of cases,  it's just that attention mechanism is getting changed  in some sense.  So I feel like definitely there are still a lot of things  to be exploring here,  and that is also one reason  why we want to keep pushing machine learning compilation,  because one of the things we are trying to push  was productivity,  so that for machine learning engineering,  so that as soon as some of the model came out,  we will be able to empower them  onto those environments that's out there. [00:55:29]

**Swyx**:  Yeah, it's a really good mission.  Okay.  Very excited to see that RWKB  and state space model stuff.  I think I'm hearing increasing chatter about that stuff.  Okay.  Podcasts, lightning rounds,  it's always fun.  So I'll take the first one.  Acceleration.  What has already happened in AI  that you thought would take much longer? [00:55:52]

**Tianqi**:  I think this emergence  of more like a conversation chatbot ability  is something that kind of surprised me  before it came out.  And now I think, yeah,  so I think this is like one piece  that I feel originally I thought would take much longer,  but yeah, it happens. [00:56:13]

**Alessio**:  Yeah. [00:56:14]

**Swyx**:  It's funny because like the original like Eliza chatbot  was something that goes all the way back in time, right?  And then we just suddenly came back again. [00:56:23]

**Tianqi**:  Yeah, it's always too interesting to come back,  but with kind of a different technology in some sense. [00:56:28]

**Alessio**:  Yes.  What about the most interesting unsolved question in AI? [00:56:36]

**Tianqi**:  Unsolved questions. [00:56:39]

**Tianqi**:  That's a hard one, right?  So I can tell you like what kind of I'm excited about.  So I think that I have always been excited  about this idea of continuous learning  and lifelong learning in some sense.  So how AI continues to evolve  with the knowledges that have been there.  It seems that we're getting much closer  with all those recent technologies.  So being able to be able to develop systems,  support, and be able to think about  how AI continues to evolve  is something that I'm really excited about. [00:57:20]

**Alessio**:  Yeah. [00:57:21]

**Swyx**:  So specifically just to double click on this,  are you talking about continuous training? [00:57:29]

**Tianqi**:  Yeah, so continuous.  Well, that's like a training.  I feel like training adaptation  and it's all similar sense, right?  You want to think about entire life cycle, right?  The life cycle of collecting data,  training, fine tuning,  and maybe have your local context  that getting continuously curated  and feed onto models.  So I think all these things are interesting and relevant. [00:57:55]

**Swyx**:  Yeah, I think this is something  that people are really asking.  Right now, we have moved a lot  into the sort of pre-training phase  and off the shelf model downloads and stuff like that,  which seems very counterintuitive  compared to the continuous training paradigm  that people want. [00:58:15]

**Alessio**:  Yeah.  Yeah, cool. [00:58:18]

**Swyx**:  And then, so I guess the last question  would be for takeaways.  What's basically one message that you want  every listener, every person to remember today? [00:58:27]

**Tianqi**:  Yeah, I think it's getting more obvious now,  but I think one of the things  I always want to mention in my talks  is that when you're thinking about AI applications,  originally, people think about algorithms a lot more.  Algorithm models, they are still very important,  but usually, when you build AI applications,  it takes both algorithm side,  the system optimizations,  and the data curations, right?  So it takes a connection of so many facades  to be able to bring together an AI system  and be able to looking at it  from that whole perspective is really useful  when we start to build modern AI applications.  I think it's going to continue  going to be more important in the future. [00:59:11]

**Alessio**:  Yeah. [00:59:12]

**Swyx**:  Yeah, well, thank you for showing the way on this,  and honestly, just making things possible  that I thought would take a lot longer.  So thanks for everything you've done.  Thank you for having me. [00:59:26]

**Alessio**:  Yeah, thanks for coming on TQ.  Have a good one.  Thank you. [00:59:30]